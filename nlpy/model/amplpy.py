"""
Python interface to the AMPL modeling language

.. moduleauthor:: M. P. Friedlander <mpf@cs.ubc.ca>
.. moduleauthor:: D. Orban <dominique.orban@gerad.ca>
"""

import numpy as np
from nlpy.model.nlp import NLPModel, SciPyNLPModel
from nlpy.model import _amplpy
from pysparse.sparse.pysparseMatrix import PysparseMatrix as sp
from pykrylov.linop import PysparseLinearOperator, CoordLinearOperator
from nlpy.tools import sparse_vector_class as sv
import warnings
import tempfile
import os

__docformat__ = 'restructuredtext'

warnings.simplefilter('always')   # Never ignore warnings.


def GenTemplate(model, data=None, opts=None):
    """
    Write out an Ampl template file,
    using files model.mod and data.dat (if available).
    The template will be given a temporary name.
    """

    # Create a temporary template file and write in a header.
    tmpname = tempfile.mktemp()
    template = open(tmpname, 'w')
    template.write("# Template file for %s.\n" % model)
    template.write("# Automatically generated by AmplPy.\n")

    # Later we can use opts to hold a list of Ampl options, eg,
    #     option presolve 0;
    # that can be written into the template file.
    if opts is not None:
        pass

    # Template file body.
    if model[-4:] == '.mod': model = model[:-4]
    template.write("model %s.mod;\n" % model)

    if data is not None:
        if data[-4:] == '.dat': data = data[:-4]
        template.write("data  %s.dat;\n" % data)
    template.write("write g%s;\n" % model)

    # Finish off the template file.
    template.close()

    # We'll need to know the template file name.
    return tmpname


def writestub(template):
    os.system("ampl %s" % template)


class AmplModel(NLPModel):
    """
    AmplModel creates an instance of an AMPL model. If the `nl` file is
    already available, simply call `AmplModel(stub)` where the string
    `stub` is the name of the model. For instance: `AmplModel('elec')`.
    If only the `.mod` file is available, set the positional parameter
    `neednl` to `True` so AMPL generates the `nl` file, as in
    `AmplModel('elec.mod', data='elec.dat', neednl=True)`.

    Among important attributes of this class are :attr:`nvar`, the number of
    variables, :attr:`ncon`, the number of constraints, and :attr:`nbounds`,
    the number of variables subject to at least one bound constraint.
    """

    def __init__(self, stub, **kwargs):

        data = kwargs.get('data', None)
        opts = kwargs.get('opts', None)

        if stub[-4:] == '.mod':
            # Create the nl file.
            template = GenTemplate(stub, data, opts)
            writestub(template)

        # Initialize the ampl module
        try:
            if stub[-4:] == '.mod': stub = stub[:-4]
            model = self.model = _amplpy.ampl(stub)
        except:
            raise ValueError('Cannot initialize model %s' % stub)

        super(AmplModel, self).__init__(model.n_var, model.n_con,
                                        name=stub,
                                        x0=model.get_x0(),
                                        pi0=model.get_pi0(),
                                        Lvar=model.get_Lvar(),
                                        Uvar=model.get_Uvar(),
                                        Lcon=model.get_Lcon(),
                                        Ucon=model.get_Ucon())

        # Get basic info on problem
        self.minimize = (model.objtype == 0)
        (self.lin, self.nln, self.net) = model.get_CType()  # Constraint types
        self.nlin = len(self.lin)           # number of linear    constraints
        self.nnln = len(self.nln)           #    ...    nonlinear   ...
        self.nnet = len(self.net)           #    ...    network     ...

        self._sparse_coord = True           # Sparse matrices in coord format

        # Get sparsity info
        self.nnzj = model.get_nnzj()        # number of nonzeros in Jacobian
        self.nnzh = model.get_nnzh()        #                       Hessian

        # Initialize scaling attributes
        self.scale_obj = None   # Objective scaling
        self.scale_con = None   # Constraint scaling

    def writesol(self, x, z, msg):
        """
        Write primal-dual solution and message msg to stub.sol
        """
        return self.model.ampl_sol(x, z, msg)

    def compute_scaling_obj(self, x=None, g_max=1.0e2, reset=False):
        """
        Compute objective scaling.

        :parameters:

            :x: Determine scaling by evaluating functions at this
                point. Default is to use :attr:`self.x0`.
            :g_max: Maximum allowed gradient. Default: :attr:`g_max = 1e2`.
            :reset: Set to `True` to unscale the problem.

        The procedure used here closely
        follows IPOPT's behavior; see Section 3.8 of

          Waecther and Biegler, 'On the implementation of an
          interior-point filter line-search algorithm for large-scale
          nonlinear programming', Math. Prog. A (106), pp.25-57, 2006

        which is a scalar rescaling that ensures the inf-norm of the
        gradient (at x) isn't larger than 'g_max'.
        """
        # Remove scaling if requested
        if reset:
            self.scale_obj = None
            self.pi0 = self.model.get_pi0()  # get original multipliers
            return

        # Quick return if the problem is already scaled
        if self.scale_obj is not None:
            return

        if x is None: x = self.x0
        g = self.grad(x)
        gNorm = np.linalg.norm(g, np.inf)
        self.scale_obj = g_max / max(g_max, gNorm)  # <= 1 always

        # Rescale the Lagrange multiplier
        self.pi0 *= self.scale_obj

        return gNorm

    def compute_scaling_cons(self, x=None, g_max=1.0e2, reset=False):
        """
        Compute constraint scaling.

        :parameters:

            :x: Determine scaling by evaluating functions at this
                point. Default is to use :attr:`self.x0`.
            :g_max: Maximum allowed gradient. Default: :attr:`g_max = 1e2`.
            :reset: Set to `True` to unscale the problem.
        """
        # Remove scaling if requested
        if reset:
            self.scale_con = None
            self.Lcon = self.model.get_Lcon()  # lower bounds on constraints
            self.Ucon = self.model.get_Ucon()  # upper bounds on constraints
            return

        # Quick return if the problem is already scaled
        if self.scale_con is not None:
            return

        m = self.m
        if x is None: x = self.x0
        d_c = np.empty(m)
        J = self.jop(x)

        # Find inf-norm of each row of J
        gmaxNorm = 0            # holds the maxiumum row-norm of J
        imaxNorm = 0            # holds the corresponding index
        e = np.zeros(self.ncon)
        for i in xrange(m):
            e[i] = 1
            giNorm = np.linalg.norm(J.T * e, 1)  # Matrix 1-norm (max abs col)
            e[i] = 0
            d_c[i] = g_max / max(g_max, giNorm)  # <= 1 always
            if giNorm > gmaxNorm:
                gmaxNorm = giNorm
                imaxNorm = i
            gmaxNorm = max(gmaxNorm, giNorm)

        self.scale_con = d_c

        # Scale constraint bounds: componentwise multiplications
        self.Lcon *= d_c        # lower bounds on constraints
        self.Ucon *= d_c        # upper bounds on constraints

        # Return largest row norm and its index

        return (imaxNorm, gmaxNorm)

###############################################################################

    # The following methods mirror the module functions defined in _amplpy.c.

    def obj(self, x, obj_num=0):
        """
        Evaluate objective function value at x.
        Returns a floating-point number. This method changes the sign of the
        objective value if the problem is a maximization problem.
        """

        # AMPL doesn't exactly exit gracefully if obj_num is out of range.
        if obj_num < 0 or obj_num >= self.model.n_obj:
            raise ValueError('Objective number is out of range.')

        f = self.model.eval_obj(x)
        if self.scale_obj: f *= self.scale_obj
        if not self.minimize: f *= -1
        return f

    def grad(self, x, obj_num=0):
        """
        Evaluate objective gradient at x.
        Returns a Numpy array. This method changes the sign of the objective
        gradient if the problem is a maximization problem.
        """

        # AMPL doesn't exactly exit gracefully if obj_num is out of range.
        if obj_num < 0 or obj_num >= self.model.n_obj:
            raise ValueError('Objective number is out of range.')

        g = self.model.grad_obj(x)
        if self.scale_obj: g *= self.scale_obj
        if not self.minimize: g *= -1
        return g

    def sgrad(self, x):
        """
        Evaluate sparse objective gradient at x.
        Returns a sparse vector. This method changes the sign of the objective
        gradient if the problem is a maximization problem.
        """
        sg = sv.SparseVector(self.n, self.model.eval_sgrad(x))
        if self.scale_obj: sg *= self.scale_obj
        if not self.minimize: sg *= -1
        return sg

    def cost(self):
        """
        Evaluate sparse cost vector.
        Useful when problem is a linear program.
        Return a sparse vector. This method changes the sign of the cost vector
        if the problem is a maximization problem.
        """
        sc = sv.SparseVector(self.n, self.model.eval_cost())
        if self.scale_obj: sc *= self.scale_obj
        if not self.minimize: sc *= -1
        return sc

    def cons(self, x):
        """
        Evaluate vector of constraints at x.
        Returns a Numpy array.

        The constraints appear in natural order. To order them as follows

        1) equalities
        2) lower bound only
        3) upper bound only
        4) range constraints,

        use the `permC` permutation vector.
        """
        c = self.model.eval_cons(x)
        if self.scale_con is not None: c *= self.scale_con
        return c

    def icons(self, i, x):
        """
        Evaluate value of i-th constraint at x.
        Returns a floating-point number.
        """
        ci = self.model.eval_ci(i, x)
        if self.scale_con is not None: ci *= self.scale_con[i]
        return ci

    def igrad(self, i, x):
        """
        Evaluate dense gradient of i-th constraint at x.
        Returns a Numpy array.
        """
        gi = self.model.eval_gi(i, x)
        if self.scale_con is not None: gi *= self.scale_con[i]
        return gi

    def sigrad(self, i, x):
        """
        Evaluate sparse gradient of i-th constraint at x.
        Returns a sparse vector representing the sparse gradient
        in coordinate format.
        """
        sci = sv.SparseVector(self.n, self.model.eval_sgi(i, x))
        if self.scale_con is not None: sci *= self.scale_con[i]
        return sci

    def irow(self, i):
        """
        Evaluate sparse gradient of the linear part of the
        i-th constraint. Useful to obtain constraint rows
        when problem is a linear programming problem.
        """
        sri = sv.SparseVector(self.n, self.model.eval_row(i))
        if self.scale_con is not None: sri *= self.scale_con[i]
        return sri

    def A(self, *args, **kwargs):
        """
        Evaluate sparse Jacobian of the linear part of the
        constraints. Useful to obtain constraint matrix
        when problem is a linear programming problem.
        """
        store_zeros = kwargs.get('store_zeros', False)
        store_zeros = 1 if store_zeros else 0
        spJac = kwargs.get('spJac', None)
        A = self.model.eval_A(self._sparse_coord, store_zeros, spJac)
        if self._sparse_coord:
          vals, rows, cols = A
          if self.scale_con is not None: vals *= self.scale_con[rows]
          return (vals, rows, cols)
        return A

    def jac(self, x, *args, **kwargs):
        """
        Evaluate sparse Jacobian of constraints at x.
        Returns a PySparse sparse matrix.
        """
        store_zeros = kwargs.get('store_zeros', False)
        store_zeros = 1 if store_zeros else 0
        J = self.model.eval_J(x, self._sparse_coord, store_zeros)
        if self._sparse_coord:
          vals, rows, cols = J
          if self.scale_con is not None: vals *= self.scale_con[rows]
          return (vals, rows, cols)
        return J

    def jac_pos(self, x, **kwargs):
        """
        Convenience function to evaluate the Jacobian matrix of the constraints
        reformulated as

            ci(x) = ai       for i in equalC
            ci(x) - Li >= 0  for i in lowerC
            ci(x) - Li >= 0  for i in rangeC
            Ui - ci(x) >= 0  for i in upperC
            Ui - ci(x) >= 0  for i in rangeC.

        The gradients of the general constraints appear in 'natural' order,
        i.e., in the order in which they appear in the problem. The gradients
        of range constraints appear in two places: first in the 'natural'
        location and again after all other general constraints, with a flipped
        sign to account for the upper bound on those constraints.

        The overall Jacobian of the new constraints thus has the form

        [ J ]
        [-JR]

        This is a `(m + nrangeC)`-by-`n` matrix, where `J` is the Jacobian
        of the general constraints in the order above in which the sign of
        the 'less than' constraints is flipped, and `JR` is the Jacobian of
        the 'less than' side of range constraints.
        """
        store_zeros = kwargs.get('store_zeros', False)
        store_zeros = 1 if store_zeros else 0
        n = self.n ; m = self.m ; nrangeC = self.nrangeC
        upperC = self.upperC ; rangeC = self.rangeC

        # Initialize sparse Jacobian
        J = sp(nrow=m + nrangeC, ncol=n, sizeHint=self.nnzj+10*nrangeC,
               storeZeros=store_zeros)

        # Insert contribution of general constraints
        J[:m, :n] = self.jac(x, **kwargs)
        J[upperC, :n] *= -1                # Flip sign of 'upper' gradients.
        J[m:, :n] = -J[rangeC, :n]     # Append 'upper' side of range const.
        return J

    # Implement jop because AMPL models don't define jprod / jtprod.
    def jop(self, x, *args, **kwargs):
        """
        Obtain Jacobian at x as a linear operator.
        """
        vals, rows, cols = self.jac(x, *args, **kwargs)
        return CoordLinearOperator(vals, rows, cols,
                                   nargin=self.nvar, nargout=self.ncon,
                                   symmetric=False)

    def hess(self, x, z=None, obj_num=0, *args, **kwargs):
        """
        Evaluate sparse lower triangular Lagrangian Hessian at (x, z).
        Returns a PySparse sparse matrix.

        By convention, the Lagrangian has the form L = f - c'z.
        """
        obj_weight = kwargs.get('obj_weight', 1.0)
        store_zeros = kwargs.get('store_zeros', False)
        store_zeros = 1 if store_zeros else 0
        spHess = kwargs.get('spHess', None)
        if z is None: z = np.zeros(self.m)
        H = self.model.eval_H(x, z, self._sparse_coord, obj_weight,
                              store_zeros, spHess=spHess)
        if self._sparse_coord:
          vals, rows, cols = H
          if self.scale_obj: vals *= self.scale_obj
          if not self.minimize: vals *= -1
          return (vals, rows, cols)
        return H

    def hprod(self, x, z, v, **kwargs):
        """
        Evaluate matrix-vector product H(x,z) * v, where H is the Hessian of
        the Lagrangian evaluated at the primal-dual pair (x,z).
        Zero multipliers can be specified as an array of zeros or as `None`.

        Returns a Numpy array.

        Bug: x is ignored, and is determined as the point at which the
        objective or gradient were last evaluated.

        :keywords:
            :obj_weight: Add a weight to the Hessian of the objective function.
                         By default, the weight is one. Setting it to zero
                         allows to exclude the Hessian of the objective from
                         the Hessian of the Lagrangian.
        """
        obj_weight = kwargs.get('obj_weight', 1.0)
        if z is None: z = np.zeros(self.m)
        Hv = self.model.H_prod(z, v, obj_weight)
        if self.scale_obj:
            Hv *= self.scale_obj
        if not self.minimize:
            Hv *= -1
        return Hv

    def hiprod(self, x, i, v, **kwargs):
        """
        Evaluate matrix-vector product Hi(x) * v.
        Returns a Numpy array.

        Bug: x is ignored. See hprod above.
        """
        z = np.zeros(self.m) ; z[i] = -1
        Hv = self.model.H_prod(z, v, 0.)
        if self.scale_con is not None:
            Hv *= self.scale_con[i]
        return Hv

    def ghivprod(self, g, v, **kwargs):
        """
        Evaluate the vector of dot products (g, Hi*v) where Hi is the Hessian
        of the i-th constraint, i=1..m.
        """
        if self.nnln == 0:           # Quick exit if no nonlinear constraints
            return np.zeros(self.m)
        gHi = self.model.gHi_prod(g, v)
        if self.scale_con is not None:
            gHi *= self.scale_con    # componentwise product
        return gHi

    def islp(self):
        """
        Determines whether problem is a linear programming problem.
        """
        if self.model.nlo or self.model.nlc or self.model.nlnc:
            return False
        return True

    def set_x(self, x):
        """
        Set `x` as current value for subsequent calls
        to :meth:`obj`, :meth:`grad`, :meth:`jac`, etc. If several
        of :meth:`obj`, :meth:`grad`, :meth:`jac`, ..., will be called with the
        same argument `x`, it may be more efficient to first call `set_x(x)`.
        In AMPL, :meth:`obj`, :meth:`grad`, etc., normally check whether their
        argument has changed since the last call. Calling `set_x()` skips this
        check.

        See also :meth:`unset_x`.
        """
        return self.model.set_x(x)

    def unset_x(self):
        """
        Reinstates the default behavior of :meth:`obj`, :meth:`grad`, `jac`,
        etc., which is to check whether their argument has changed since the
        last call.

        See also :meth:`set_x`.
        """
        return self.model.unset_x()

    def display_basic_info(self):
        """
        Display vital statistics about the current model.
        """
        super(AmplModel, self).display_basic_info()

        # Display info that wasn't available in NLPModel.
        write = self.logger.info
        write('Number of nonzeros in Jacobian: %d\n' % self.nnzj)
        write('Number of nonzeros in Lagrangian Hessian: %d\n' % self.nnzh)
        if self.islp(): write('This problem is a linear program.\n')

        return


class PySparseAmplModel(AmplModel):
  # Here, there's no reason to inherit from PySparseNLPModel
  # as we'll never revert to it. Instead, we reimplement `jac`
  # and `hess` because the AMPL module # can return a PySparse
  # matrix directly, which is more efficient.

  def __init__(self, *args, **kwargs):
    super(PySparseAmplModel, self).__init__(*args, **kwargs)
    self._sparse_coord = False

  def A(self, *args, **kwargs):
    """
    Evaluate sparse Jacobian of the linear part of the
    constraints. Useful to obtain constraint matrix
    when problem is a linear programming problem.
    """
    A = super(PySparseAmplModel, self).A(*args, **kwargs)
    if self.scale_con is not None: A.row_scale(self.scale_con)
    return A

  def jac(self, x, *args, **kwargs):
    """
    Evaluate sparse Jacobian of constraints at x.
    Returns a PySparse sparse matrix.
    """
    J = super(PySparseAmplModel, self).jac(x, *args, **kwargs)
    if self.scale_con is not None: J.row_scale(self.scale_con)
    return J

  def jop(self, x, *args, **kwargs):
    """
    Obtain Jacobian at x as a linear operator.
    """
    return PysparseLinearOperator(self.jac(x, *args, **kwargs))

  def hess(self, x, z=None, obj_num=0, *args, **kwargs):
    """
    Evaluate sparse lower triangular Lagrangian Hessian at (x, z).
    Returns a PySparse sparse matrix.

    By convention, the Lagrangian has the form L = f - c'z.
    """
    H = super(PySparseAmplModel, self).hess(x, z, obj_num, *args, **kwargs)
    if self.scale_obj: H.scale(self.scale_obj)
    if not self.minimize: H.scale(-1)
    return H


class SciPyAmplModel(SciPyNLPModel, AmplModel):
  # MRO: 1. SciPyAmplModel
  #      2. SciPyNLPModel
  #      3. AmplModel
  #      4. NLPModel

  def A(self, *args, **kwargs):
      """
      Evaluate sparse Jacobian of the linear part of the
      constraints. Useful to obtain constraint matrix
      when problem is a linear programming problem.
      """
      vals, rows, cols = super(SciPyAmplModel. self).A(*args, **kwargs)
      return sp.coo_matrix((vals, (rows, cols)),
                           shape=(self.ncon, self.nvar))

  # Here, `jac` and `hess` are inherited directly from SciPyNPLModel.
